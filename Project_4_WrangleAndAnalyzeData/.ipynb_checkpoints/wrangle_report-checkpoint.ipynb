{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs - Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data wrangling project, I gathered, assessed and cleaned the data taken from multiple sources and then provided a basic insight on the data. In this report, I will explain the efforts taken in the 3 steps of Data Wrangling:\n",
    "<br>1) Gather\n",
    "<br>2) Assess\n",
    "<br>3) Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Gather\n",
    "The first step towards Data wrangling is to gather data. In this project, I had to gather data from 3 sources, each having its own usecase and significance.\n",
    "<br>\n",
    "<br>The first piece of data that was gathered was the WeRateDogs Twitter archive data. This was taken from **twitter_archive_enhanced.csv** (which was already provided to us). This csv was uploaded to the jupyter notebook workspace and was then read into a pandas dataframe called **df_twitter_archive_enhanced**\n",
    "<br>\n",
    "<br>The second piece of information was the image predictions of the tweets. This data was available in one of Udacity's server and was pulled using the **Requests** library in python and stored in a pandas dataframe called **df_image_predictions**. This data was also exported to **image_predictions.tsv**\n",
    "<br>\n",
    "<br>The third piece of information that was gathered was the retweet count and favorite count for these tweets using **Twitter APIs**. This was done using the **Tweepy** library. But, in order to query the twitter APIs, I first got a developer account using which i created an app. To use the app, I recieved the keys using which i was able to query the twitter APIs and store the gathered information in a text file **tweet_json.txt**. The data was then read into a pandas dataframe called **df_tweet_data** from this text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Assess\n",
    "\n",
    "In the assessment phase of data wrangling, I made use of both the techniques - visual inspection and assessment through code. Most of the issues I found in the data were through assessment by code. \n",
    "<br> I made use of pandas's inbuilt functions such as info(), describe(), sample(), head(), tail() and so on to spot quality and tideness issues in each of the 3 dataframes. \n",
    "The following quality and tideness issues were found in the collected data\n",
    "\n",
    "#### Quality\n",
    "\n",
    "**df_twitter_archive_enhanced**\n",
    "<br>1) Columns doggo, floofer, puppo, and pupper should be boolean but have either 'None' or the column name themselves stored. \n",
    "<br>2) Timestamp column is in string format. Must be datetime \n",
    "<br>3) The Dog Name column has 'None' as an entry. Should be Nan \n",
    "<br>4) The \"in_reply_to_status_id\" and \"in_reply_to_user_id\" columns are in float. Must be in string format as they are just IDs and not numbers to manipulate \n",
    "<br>5) Unwanted data from columns retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp \n",
    "<br>6) Dog names column has wrong entries such as 'a', 'actually', 'all', 'an', 'by', 'getting', 'his' etc \n",
    "<br>7) Some wrong rating_numerators and rating_denominators were recorded from the text (misinterpretations)\n",
    "\n",
    "**df_image_predictions**\n",
    "\n",
    "8) p1, p2 and p3 columns must have only dog breeds, but contain other data as well\n",
    "\n",
    "**Tideness**\n",
    "\n",
    "**df_twitter_archive_enhanced**\n",
    "<br>1) Source column of df_twitter_archive_enhanced is unwanted \n",
    "<br>2) All Dataset can be combined into a single dataset for analysis purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Clean\n",
    "\n",
    "The last step of data wrangling is to clean the data. I followed the 3 step process to clean the data for each of the above identified issues:\n",
    "<br>1) Define\n",
    "<br>2) Code\n",
    "<br>3) Test\n",
    "<br>As seen above, the data was quite dirty and untidy. It had issues such as unwanted information, incorrect data, incorrect data types, inaccurate information, scattered data and so on. The goal was to clean this data so that it can be used to give some sensible insights\n",
    "<br>Most of the data was cleaned using pandas functions such as drop(), dropna(), astype(), replace() and so on. These functions helped me easily clean the data step by step and in the end, I saved the data in a single dataframe called **df_we_rate_dogs** and also saved it in a csv file called **twitter_archive_master.csv**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
